
# Ответы на вопросы из ДЗ

## Загрузка файла

> На сервере лежит файл размером в несколько гигабайтов. Как его вытащить и эффективно и записать в файл? 
> Я так понимаю, что реквест полностью подгружает всю информацию из файла на сервере, что не эффективно.
> Хотелось бы как-то буфферизацию или сжатие прикрутить. Какой вообще стандарт для таких ситуаций. 
> Объясните по-подробнее пожалуйста.

В качестве примера приводится код:

```go
package main

import (
    "fmt"
    "io"
    "net/http"
    "os"
)

func GetFileFromUrl(url string) (string, error) {
    // Make the HTTP request
    resp, err := http.Get(url)
    if err != nil {
        return "", fmt.Errorf("error fetching the URL: %w", err)
    }
    defer resp.Body.Close()

    // Check if the response status is OK (200)
    if resp.StatusCode != http.StatusOK {
        return "", fmt.Errorf("error: received non-200 response status: %s", resp.Status)
    }

    // Create a temporary file
    tempFile, err := os.CreateTemp("", "logfile-*.log")
    if err != nil {
        return "", fmt.Errorf("error creating temporary file: %w", err)
    }
    defer tempFile.Close()

    // Write the response body to the temporary file
    _, err = io.Copy(tempFile, resp.Body)
    if err != nil {
        return "", fmt.Errorf("error writing to temporary file: %w", err)
    }

    // Return the path to the temporary file
    return tempFile.Name(), nil
}

// Example usage
func main() {
    filePath, err := GetFileFromUrl("https://example.com/somefile")
    if err != nil {
        fmt.Println("Error:", err)
        return
    }

    fmt.Println("File saved to:", filePath)
}
```

Стоит посмотреть на реализацию функции `io.Copy`. Она, как и аналогичная функция `io.CopyBuffer`,
использует байтовый буфер фиксированного размера (длина по умолчанию = 32 * 1024 байт) для копирования данных.

Но если переданные в эту функцию интерфейсы имеют функции-ресиверы `ReadFrom` и `WriteTo`, то вместо
байтового буфера будут использованы они. 

Если в данной ОС поддерживается системный вызов [copy_file_range](https://man7.org/linux/man-pages/man2/copy_file_range.2.html),
то функция `io.Copy` будет использовать `ReadFrom` для файла (тип `os.File`). В конечном итоге, всё содержимое действительно
будет загружено в память, что может быть нежелательно. Но если данный системный вызов в ОС не поддерживается,
то функция `io.Copy/CopyBuffer` будет работать нужным в данном случае образом: с использованием буфера.

Можно гарантировать, что чтение и/или запись будут буферизированными, с помощью функций пакета `bufio`:

```go
const bufferSize = 100_000_000
_, err = io.Copy(
    bufio.NewWriterSize(tempFile, bufferSize),
    bufio.NewReaderSize(resp.Body, bufferSize),
)
```

Также пакет `bufio` реализует тип `Scanner`. Можно переписать код, используя `bufio.Scanner`:

```go
package main

import (
    "bufio"
    "fmt"
    "io"
    "net/http"
)

func YieldLinesFromReader(reader io.Reader, bufferLimit int, linesReadLimit int) chan string {
    result := make(chan string, linesReadLimit)

    go func() {
        defer close(result)

        scanner := bufio.NewScanner(
            bufio.NewReaderSize(
                reader,
                bufferLimit,
            ),
        )
        for scanner.Scan() {
            result <- scanner.Text()
        }
    }()

    return result
}

func ProcessLinesFromUrl(url string) error {
    resp, err := http.Get(url)
    if err != nil {
        return fmt.Errorf("error fetching the URL: %w", err)
    }
    defer func() { _ = resp.Body.Close() }()

    if resp.StatusCode != http.StatusOK {
        return fmt.Errorf("error fetching the URL: %s", resp.Status)
    }

    const (
        bufferLimit  = 100_000_000
        linesChanCap = 5
    )
    for line := range YieldLinesFromReader(resp.Body, bufferLimit, linesChanCap) {
        fmt.Println(line)
    }

    return nil
}

func main() {
    const url = "https://example.com/somefile"

    if err := ProcessLinesFromUrl(url); err != nil {
        fmt.Println("couldn't process url: ", err)
    }
}
```